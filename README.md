# Go KV Store (High-Performance Redis Compatible)

![Go Version](https://img.shields.io/badge/Go-1.23+-00ADD8?logo=go&logoColor=white)
![Performance](https://img.shields.io/badge/Performance-9.2M_RPS-brightgreen)
![Protocol](https://img.shields.io/badge/Protocol-RESP-red)

In-memory key-value хранилище на Go, реализующее протокол Redis (RESP).

Проект создавался как исследование производительности Go. В результате серии низкоуровневых оптимизаций (*zero-copy, syscall batching, sharding*) удалось достичь производительности в 9,200,000+ запросов в секунду (GET) и 5,500,000+ запросов в секунду (SET) в режиме pipeline.

> Ключевая особенность: Сервер оптимизирован для минимизации системных вызовов (syscall), что критично для работы на Windows/WSL и обеспечивает максимальную пропускную способность (throughput).

---

## Бенчмарки

Замеры проводились на localhost (Windows, 12 threads), используя собственный бенчмарк-клиент с поддержкой агрессивного пайплайнинга.

| Mode | Operation | Batch Size | Throughput (RPS) | Latency (avg) |
|:---|:---|:---|---:|---:|
| Pipeline (Max) | GET | 20,000 | 9,200,000 | 20.7 ms |
| Pipeline (Max) | SET | 20,000 | 5,500,000 | 49.8 ms |
| Pipeline (Realistic) | GET | 100 | 400,000 | 4 ms |
| Pipeline (Realistic) | SET | 100 | 200,000 | 8 ms |
| Request-Response | GET | 1 | 26,000 | 38 µs |
| Request-Response | SET | 1 | 27,600 | 36 µs |

> *Примечание:* Высокая латентность (ms) в режиме Pipeline Max (Batch 20k) обусловлена огромным размером батча. Это сознательный компромисс для демонстрации максимальной пропускной способности (CPU Throughput) и проверки пределов движка.
>
> На Linux при малых батчах (например, 100) скорость обычно заметно выше, чем на Windows, из-за меньшей стоимости системных вызовов. При батчах 20k разница почти исчезает, так как количество syscalls минимально.

---

## Engineering Journey: Как мы ускорили Go в 300 раз

Изначально сервер выдавал около 27k RPS. Профилирование показало интересную картину.

### 1. Проблема: "Налог на Windows"
Запуск pprof выявил, что 80% процессорного времени тратилось не на логику базы данных, а на общение с ОС:
* 64% — runtime.cgocall (переключение контекста Go -> C -> Kernel).
* 16% — runtime.stdcall (работа I/O completion ports).
* Полезная работа (storage.Set, resp.Parse) занимала менее 20%.

На Windows системные вызовы стоят очень дорого. Стандартный подход *"один Write на один ответ"* убивал производительность, так как процессор занимался только переключением контекста.

### 2. Решение: Smart Batching & Syscall Reduction
Мы переписали сетевой слой (handler.go), внедрив агрессивную буферизацию:
* Умный Flush: Сервер не отправляет данные в сеть сразу. Он накапливает ответы в буфере (до 64KB+).
* Адаптивный сброс: syscall.WSASend вызывается только в двух случаях:
   1. Буфер ответов переполнен.
   2. Клиент перестал слать новые команды (входящий буфер пуст).
* Результат: Вместо 20,000 системных вызовов на батч мы делаем 1-2 вызова.

Итог: Нагрузка на runtime.cgocall упала практически до нуля. CPU переключился на полезную работу (парсинг и хеш-таблицы), что дало прирост с 27k до 9.2M RPS.

---

Ключевые оптимизации
1. Sharding (FNV-1a Hashing)
Вместо одного глобального sync.Mutex, который выстраивал все горутины в очередь, хранилище разбито на 16 шардов.
* Для выбора шарда используется быстрый хеш-алгоритм FNV-1a.
* Каждый шард защищен своим sync.RWMutex.
* Результат: Операции чтения и записи по разным ключам выполняются параллельно, не блокируя друг друга.
2. Zero-Copy & Unsafe
Мы агрессивно боролись с нагрузкой на Garbage Collector (GC), убирая лишние аллокации:
* unsafe.StringData(): Используется для вычисления хеша ключа прямо из байтового буфера без создания временной строки (копирования памяти).
* bufio.ReadSlice: Чтение данных из сокета происходит без копирования в новые слайсы.
* writeIntFast: Числа записываются в буфер через стек, без использования тяжелого strconv.Itoa.
3. Memory Recycling
* Буферы ввода/вывода: Каждое соединение имеет свои закрепленные буферы (Reader/Writer), которые не пересоздаются.
* Слайс аргументов: Массив args для парсинга команд переиспользуется через args[:0]. Это снижает давление на GC практически до нуля даже при миллионах RPS.

---

## Tech Stack

* Language: Go 1.23+
* Core: net, sync, unsafe
* Protocol: RESP (Redis Serialization Protocol)
* Profiling: net/http/pprof, go tool pprof

---

## ⌨️ Поддерживаемые команды

Сервер поддерживает базовое подмножество команд Redis, необходимое для работы в качестве кеша или KV-хранилища.

| Команда | Описание | Пример |
|:---|:---|:---|
| `SET key value` | Установить значение ключа | `SET user:1 "John"` |
| `GET key` | Получить значение ключа | `GET user:1` |
| `PING` | Проверка соединения | `PING` |
| `QUIT` / `EXIT` | Закрыть соединение | `QUIT` |
| `CONFIG GET` | Совместимость с бенчмарками | `CONFIG GET *` |

---

## Запуск и Тестирование

### 1. Запуск сервера
```bash
go run ./cmd/server
```

### 2. Запуск бенчмарка
Встроенный бенчмарк для проверки пиковой производительности:
```bash
go run -tags benchmark ./bench -pipeline-only -pipeline-batch 20000
```

### 3. Проверка через redis-cli
Проект совместим со стандартным клиентом Redis:
```bash
redis-cli -p 6379 SET mykey "Hello World"
redis-cli -p 6379 GET mykey
# "Hello World"
```

---

## Структура проекта
```
go-kv-store/
├── cmd/
│   └── server/
│       └── main.go      # Entry point
├── internal/
│   ├── handler/
│   │   └── handler.go   # Сетевой слой и Smart Flushing
│   ├── resp/
│   │   └── resp.go      # Оптимизированный RESP парсер
│   └── storage/
│       └── shard.go     # Шардированная Map (In-memory DB)
├── bench/
│   ├── benchmark.go     # High-load бенчмарк (pipeline)
│   ├── benchmark_test.go
│   └── client.go
├── go.mod
└── README.md
```

---
