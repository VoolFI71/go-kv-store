# GoKVStore

**In-memory key-value хранилище на Go по протоколу Redis**

> Полноценный сервер, совместимый с официальными клиентами Redis и оптимизированный под экстремальные нагрузки. Благодаря инженерным оптимизациям достигнуто более **420 тысяч запросов в секунду** в pipeline режиме.

## Стек и особенности

### Tech Stack

* **Language**: Go 1.23+ (Goroutines, Unsafe)
* **Протокол**: RESP — подходит любой Redis-клиент
* **Параллелизм**: 16 шардов (FNV-1a), RWMutex на шард
* **Память**: zero-copy парсинг, переиспользование буферов, выборочное использование unsafe
* **Сеть**: TCP pipelining — батчи команд без ожидания ответа на каждый

## Бенчмарки

Замеры: встроенный бенчмарк и `redis-benchmark`, локально, 10 клиентов.

| Mode | Operation | Throughput (RPS) | Latency (avg) | Latency (p99) |
|---|---|---|---|---|
| Request-Response | GET (10 clients) | **49,963** | 199.12µs | 836.9µs |
| Request-Response | SET (10 clients) | **~50,000** | 267.9µs | 997.1µs |
| TCP Pipelining | GET (batch=100) | **420,387** | 2.32ms | 4.02ms |
| TCP Pipelining | SET (batch=100) | **210,254** | 4.61ms | 8.86ms |

### Условия
- Go 1.23+, 10 клиентов, по 100k операций на сценарий, batch=100 в pipeline
- Инструменты: свой бенчмарк и `redis-benchmark`

Цифры сняты на localhost — так видно именно работу парсера и шардов без влияния сети.

## Что ускоряли по шагам

### 1. Sharding вместо одной блокировки

Один `sync.Mutex` на всё хранилище — все горутины стоят в очереди даже при работе с разными ключами. Сделали 16 шардов по хешу ключа (FNV-1a), у каждого свой RWMutex. Операции по разным ключам идут параллельно.

### 2. Меньше копирований и аллокаций

`string`→`[]byte` и `strconv.Itoa` давали миллионы аллокаций в секунду и лишнюю нагрузку на GC. Добавили:
- `unsafe.StringData()` + `unsafe.Slice()` в `GetShardIndex` — хеш считаем без копирования ключа
- `writeIntFast` — число пишем в буфер с фиксированным массивом на стеке, без строк
- `ReadSlice('\n')` вместо `ReadBytes` — лишних копий нет

Дало +12–29% в разных тестах. В `ParseBulkString` unsafe не используем: буфер общий между командами, строки уходят в мапу — перезапись буфера испортила бы уже сохранённые значения. Там оставили обычное копирование.

### 3. Pipeline по сети

Когда клиент ждёт ответ на каждый запрос, упираемся в RTT — выходило ~25–50k RPS. Сделали батчи: клиент шлёт пачку команд, сервер обрабатывает подряд без лишних `read`/`write`. GET по pipeline — **420k RPS**, примерно в 8 раз выше, чем по одному запросу.

### 4. Переиспользование буферов

Новый слайс на каждую команду — лишние аллокации и давление на GC. Буфер 64KB на соединение и один слайс аргументов с `args[:0]` и запасом по capacity. Меньше мусора — ниже задержки на высоких перцентилях.

## Производительность: текущее состояние и планы оптимизации

### Текущее состояние

По результатам профилирования (pprof) на Windows выявлены следующие узкие места:

**Распределение времени CPU:**
- **66.48%** — системные вызовы Windows (`runtime.cgocall`, `syscall.SyscallN`)
- **50%** — запись данных в сеть (`bufio.Writer.Flush`, `syscall.WSASend`)
- **30%** — планировщик горутин (`runtime.schedule`, `runtime.park_m`)
- **29%** — сетевой polling (`runtime.netpoll`)
- **18%** — чтение данных (`bufio.Reader.ReadSlice`, `ParseArray`)
- **1%** — доступ к хранилищу (`Storage.Get`)

### Почему такая задержка?

**Основная проблема: частые flush'и буфера записи**

Текущая логика flush работает так:
- После каждой команды вызывается `flushIfBatchDone()`
- Flush происходит только если:
  * Накопилось **128 ответов** (принудительно)
  * ИЛИ прошло **16 ответов** И буфер чтения пуст (пачка обработана)
- В pipeline режиме клиент шлет батчи команд (например, по 100 команд)
- После обработки батча буфер чтения часто пуст, поэтому flush срабатывает **каждые 16 ответов**

**Почему это проблема на Windows:**
- Каждый `Flush()` вызывает системный вызов `WSASend`
- На Windows системные вызовы очень дорогие из-за переключения контекста между пользовательским и ядерным пространством
- При flush каждые 16 ответов это дает ~6,250 flush'ей на 100k операций
- Каждый flush = ~1-2 микросекунды overhead, что накапливается

**Хорошие новости:**
- Доступ к хранилищу занимает всего 1% времени — шардирование работает отлично
- Парсинг RESP эффективен (18% времени) — zero-copy оптимизации работают
- Планировщик Go эффективно использует CPU (30% времени на планирование — нормально для высокой нагрузки)

**Важно:** На Windows системные вызовы всегда будут узким местом для сетевых приложений. Это фундаментальное ограничение платформы. Оптимизации могут снизить количество вызовов, но не могут их полностью устранить.

## Структура

Обычный расклад под Go (cmd / internal / отдельный bench):

```
go-kv-store/
├── cmd/
│   └── server/          # Точка входа сервера
│       └── main.go
├── internal/
│   ├── handler/         # Обработка соединений и команд
│   │   └── handler.go
│   ├── resp/            # Парсинг и сериализация RESP протокола
│   │   └── resp.go
│   └── storage/          # Шардированное хранилище
│       └── shard.go
├── bench/                # Бенчмарк инструменты
│   ├── client.go
│   ├── benchmark.go
│   └── benchmark_test.go
├── go.mod
└── README.md
```

## Архитектура

```
┌─────────────┐
│   Client    │
│ (redis-cli) │
└──────┬──────┘
       │ RESP Protocol
       ▼
┌─────────────────────────────────┐
│      TCP Listener (6379)        │
└──────┬──────────────────────────┘
       │
       ▼
┌─────────────────────────────────┐
│     RESP Parser (Zero-Copy)     │
│  - ReadSlice для чтения         │
│  - parseInt для чисел           │
│  - Buffer recycling             │
└──────┬──────────────────────────┘
       │
       ▼
┌─────────────────────────────────┐
│    Command Handler              │
│  - Direct string comparison     │
│  - No ToUpper overhead          │
└──────┬──────────────────────────┘
       │
       ▼
┌─────────────────────────────────┐
│   ShardStorage (16 shards)      │
│  ┌────────┐ ┌────────┐         │
│  │ Shard 0│ │ Shard 1│ ...      │
│  │ RWMutex│ │ RWMutex│         │
│  └────────┘ └────────┘         │
└─────────────────────────────────┘
```

## Запуск

Нужен Go 1.23+. Для проверки через Redis — установленный `redis-cli`.

```bash
go run ./cmd/server
```

Слушает `0.0.0.0:6379`.

### Проверка (redis-cli)

```bash
# Установить значение
redis-cli -p 6379 SET mykey "Hello World"

# Получить значение
redis-cli -p 6379 GET mykey
# Output: "Hello World"

# Проверить несуществующий ключ
redis-cli -p 6379 GET nonexistent
# Output: (nil)

# Ping команда
redis-cli -p 6379 PING
# Output: PONG
```

### Бенчмарк

Сервер должен уже крутиться на 6379.

```bash
go run -tags benchmark ./bench
```

Или через redis-benchmark:

```bash
redis-benchmark -h localhost -p 6379 -n 100000 -c 50 -t set,get
redis-benchmark -h localhost -p 6379 -n 1000000 -c 50 -t set,get -P 100   # pipeline, batch 100
```

## Команды

| Команда | Описание | Пример |
|---|---|---|
| `SET key value` | Установить значение ключа | `SET name "John"` |
| `GET key` | Получить значение ключа | `GET name` |
| `PING` | Проверка соединения | `PING` |
| `QUIT` / `EXIT` | Закрыть соединение | `QUIT` |
| `CONFIG GET` | Совместимость с redis-benchmark | `CONFIG GET *` |

## Под капотом

### Шарды
16 шардов, ключ — `hash(key) % ShardCount`, на шард свой RWMutex. Запросы к разным ключам не блокируют друг друга.

### RESP
Поддерживаются массивы команд, bulk/simple strings, ошибки, `$-1`. Работают `redis-cli` и redis-benchmark.

### Память и парсинг
- 64KB буфер на соединение, один слайс аргументов с `[:0]`
- Хеш ключа через `unsafe.StringData()` — без копирования, только чтение
- `writeIntFast` пишет число в буфер без строк и аллокаций
- Чтение через `ReadSlice`, без лишних копий

### Кратко по оптимизациям
Шарды · zero-copy парсинг · unsafe только для «одноразового» хеша · переиспользование буферов · pipeline по сети · запись чисел без `strconv.Itoa`